{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Local LLM Plugin? Local LLM Plugin allows to load a large language model (LLM) of GGUF format and run it on the Unreal Engine. Run locally and within BP/C++ Runs offline on a local PC. Just add one component to your BP and you are ready to use it. No Python or dedicated server is required. Useful features Works asynchronously, additional questions can be asked at any time during answer generation. Supports saving and loading of \"state\" that preserve the context of a conversation with an LLM, allowing you to resume a previous conversation later. Supports multibyte characters.","title":"What is Local LLM Plugin?"},{"location":"#what-is-local-llm-plugin","text":"Local LLM Plugin allows to load a large language model (LLM) of GGUF format and run it on the Unreal Engine.","title":"What is Local LLM Plugin?"},{"location":"#run-locally-and-within-bpc","text":"Runs offline on a local PC. Just add one component to your BP and you are ready to use it. No Python or dedicated server is required.","title":"Run locally and within BP/C++"},{"location":"#useful-features","text":"Works asynchronously, additional questions can be asked at any time during answer generation. Supports saving and loading of \"state\" that preserve the context of a conversation with an LLM, allowing you to resume a previous conversation later. Supports multibyte characters.","title":"Useful features"},{"location":"changelog/","text":"Changelog v1.1.1 (Jul. 1, 2024) Changed the default context length from 2048 to -1 (the context length the model trained) Added workaround for error when saving states of Gemma 2 model v1.1.0 (Jun. 30, 2024) Added support for Gemma 2. (llama.cpp runtime updated to b3262) v1.0.0 (Jun. 11, 2024) First release.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v111-jul-1-2024","text":"Changed the default context length from 2048 to -1 (the context length the model trained) Added workaround for error when saving states of Gemma 2 model","title":"v1.1.1 (Jul. 1, 2024)"},{"location":"changelog/#v110-jun-30-2024","text":"Added support for Gemma 2. (llama.cpp runtime updated to b3262)","title":"v1.1.0 (Jun. 30, 2024)"},{"location":"changelog/#v100-jun-11-2024","text":"First release.","title":"v1.0.0 (Jun. 11, 2024)"},{"location":"demo/","text":"Chat demo Download Packaged build in EXE format is available here GitHub source UE5 project files are available at GitHub Software Requirements Windows 64bit Unreal Engine 5.4.2 Local LLM plugin v1.0 or above (Optinal) if you want to run with a GPU, CUDA: 12.2.0 Hardware Reauirements A CPU that supports AVX, AVX2 and FMA. The following CPUs should work. Intel: 4th Generation (Haswell) and above AMD: All Ryzen series (Optinal) if you want to run with a GPU, A NVIDIA GPU that supports CUDA 12.2.0 How to use this demo Demo project contains 2 maps. /Content/LocalLLMDemo/Map/ChatDemo_One Demonstration of conversation with 1 NPC /Content/LocalLLMDemo/Map/ChatDemo_Multi Demonstration of conversation with 2 NPCs When you get close to the characters, a dialog UI automatically appears.","title":"Chat demo"},{"location":"demo/#chat-demo","text":"","title":"Chat demo"},{"location":"demo/#download","text":"Packaged build in EXE format is available here","title":"Download"},{"location":"demo/#github-source","text":"UE5 project files are available at GitHub","title":"GitHub source"},{"location":"demo/#software-requirements","text":"Windows 64bit Unreal Engine 5.4.2 Local LLM plugin v1.0 or above (Optinal) if you want to run with a GPU, CUDA: 12.2.0","title":"Software Requirements"},{"location":"demo/#hardware-reauirements","text":"A CPU that supports AVX, AVX2 and FMA. The following CPUs should work. Intel: 4th Generation (Haswell) and above AMD: All Ryzen series (Optinal) if you want to run with a GPU, A NVIDIA GPU that supports CUDA 12.2.0","title":"Hardware Reauirements"},{"location":"demo/#how-to-use-this-demo","text":"Demo project contains 2 maps. /Content/LocalLLMDemo/Map/ChatDemo_One Demonstration of conversation with 1 NPC /Content/LocalLLMDemo/Map/ChatDemo_Multi Demonstration of conversation with 2 NPCs When you get close to the characters, a dialog UI automatically appears.","title":"How to use this demo"},{"location":"faq/","text":"FAQ What is the GGUF format? It is a model format used in a machine learning framework called GGML published by Georgi Gerganov. One of its features is that information required for language models, such as vocabulary definitions, is included in a single file, making it highly portable. For details, please see the official page . Do I need a GPU? No, GPU is optional and it works fine with CPU only.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#what-is-the-gguf-format","text":"It is a model format used in a machine learning framework called GGML published by Georgi Gerganov. One of its features is that information required for language models, such as vocabulary definitions, is included in a single file, making it highly portable. For details, please see the official page .","title":"What is the GGUF format?"},{"location":"faq/#do-i-need-a-gpu","text":"No, GPU is optional and it works fine with CPU only.","title":"Do I need a GPU?"},{"location":"how-to-get-model/","text":"Supported models This plugin uses GGUF format models. Download a model and place it to a desired path. The following models have been tested to work. Llama 3 8B Phi-3 Medium Gemma 7B Gemma-2 9B Mistral 7B (Japanese model) ArrowPro 7B RobinHood ArrowPro 7B KUJIRA ELYZA JP 8B","title":"Supported models"},{"location":"how-to-get-model/#supported-models","text":"This plugin uses GGUF format models. Download a model and place it to a desired path. The following models have been tested to work. Llama 3 8B Phi-3 Medium Gemma 7B Gemma-2 9B Mistral 7B (Japanese model) ArrowPro 7B RobinHood ArrowPro 7B KUJIRA ELYZA JP 8B","title":"Supported models"},{"location":"how-to-save/","text":"State save and load You can save and load \u201cstates\u201d that hold the current settings and conversation history. Save to / Load from Asset Call SaveStateToAsset function to save the state asset to the specified path. Call LoadState function to load the state from the specified asset. All the current settings and conversation history are discarded. The model is re-initialized with the loaded settings and the conversation history. Save to / Load from Slot As in the usual Save Game , state can be saved to a slot specified by the Slot Name and User Index . Call SaveStateToSlot function to save the state to the specified slot. Call LoadStateFromSlot function to load the state from the specified slot. All the current settings and conversation history are discarded. The model is re-initialized with the loaded settings and the conversation history. Use Save Game objects directly To use Save Game objects directly with Create Save Game Object or Load Game from Slot , you can save and load state as described above using the following two functions. Save State function: Writes state to the specified LocalLLMSaveState object. Load State function: Read state from the specified LocalLLMSaveState object. Initialize from state saves To initialize the LLM with saved states, select the Local LLM component and make the following settings in the Local LLM > Initialization Method in the Details tab. Initialization Method = Saved State : Use to initialize by loading a saved state from an asset. Specify asset to load by Saved State . Initialization Method = Saved State from Slot : Use to initialize by loading a saved state from a slot. Specify the slot to load by Slot Name and User Index . Especially if the Initial Prompt and Talk History are long, loading the saved state will initialize the LLM faster than initializing the LLM in the usual way.","title":"State save and load"},{"location":"how-to-save/#state-save-and-load","text":"You can save and load \u201cstates\u201d that hold the current settings and conversation history.","title":"State save and load"},{"location":"how-to-save/#save-to-load-from-asset","text":"Call SaveStateToAsset function to save the state asset to the specified path. Call LoadState function to load the state from the specified asset. All the current settings and conversation history are discarded. The model is re-initialized with the loaded settings and the conversation history.","title":"Save to / Load from Asset"},{"location":"how-to-save/#save-to-load-from-slot","text":"As in the usual Save Game , state can be saved to a slot specified by the Slot Name and User Index . Call SaveStateToSlot function to save the state to the specified slot. Call LoadStateFromSlot function to load the state from the specified slot. All the current settings and conversation history are discarded. The model is re-initialized with the loaded settings and the conversation history. Use Save Game objects directly To use Save Game objects directly with Create Save Game Object or Load Game from Slot , you can save and load state as described above using the following two functions. Save State function: Writes state to the specified LocalLLMSaveState object. Load State function: Read state from the specified LocalLLMSaveState object.","title":"Save to / Load from Slot"},{"location":"how-to-save/#initialize-from-state-saves","text":"To initialize the LLM with saved states, select the Local LLM component and make the following settings in the Local LLM > Initialization Method in the Details tab. Initialization Method = Saved State : Use to initialize by loading a saved state from an asset. Specify asset to load by Saved State . Initialization Method = Saved State from Slot : Use to initialize by loading a saved state from a slot. Specify the slot to load by Slot Name and User Index . Especially if the Initial Prompt and Talk History are long, loading the saved state will initialize the LLM faster than initializing the LLM in the usual way.","title":"Initialize from state saves"},{"location":"how-to-use/","text":"Basic usage See Plugins > LocalLLM > BP > BP_LocalLLM_Example for a sample implementation. You can test it in sample maps located at Plugins > LocalLLM > Map . 1. Add a component Create an actor blueprint. Add Local LLM component. 2. Edit settings Select the Local LLM component and edit the following settings at Local LLM > Local LLM Setting in the Details tab. Model Setting Path Type : The path type of the model. Whether absolute path or relative to the project directories. Model Path : Specify the path to the gguf model. Execution device : Whether to use CPU or GPU. Number Of Gpu Layers : If using a GPU, specify the number of layers in the model to be run on the GPU. Prompt Setting Initial Prompt : Describe the role of the LLM and basic instructions. Talk History : Describe the history of the conversation so far. This is used when you want to resume a previous conversation without using the state save/load described in the next page. Both of these are \"the first prompt entered into the LLM\" when the LLM is initialized. The difference is that the Initial Prompt is retained throughout the conversation (in the default setting), while the Talk History can be discarded if the conversation becomes too long. This is because there is a limit to the length of the string (context length) that can be entered into the LLM. How information is retained and discarded is set in the Context Setting described later. Example of Initial Prompt Role play. You are a talented engineer. You speak as an \"Assistant\". You give appropriate advice in response to the statement of \"User\". Your advice should be brief and to the point. An example of a conversation is shown below. (Example) User: I'm getting an error message and the compile fails. Assistant: Read the error message first. Most things can be solved with that. User: I have no idea what it's saying! Assistant: Then Google the error message. You may be able to find a solution in the forum. User: OK! Assistant: It is also important to read the manual, if there is one! Let's get started. Interaction Setting Set how to process prompts for chat-like experience with LLMs. Templates for common settings can be selected at Interaction Template . Interaction Template = Interaction of Two : Automatically configures the settings for the conversation between two people, the User and the Assistant. The names of the User and AI Assistant must be set, and these must match the conversation examples, if any, described in the Prompt Setting . Interaction Template = Instruction : Configures the settings for Instruction format. Used for Alpaca models, etc. Interaction Template = ChatML : Configures the settings for ChatML format. Used for models trained with ChatML syntax. Manual settings without templates If Interaction Template = None , set the following values manually. Antiprompts : When LLM outputs one of these phrases, it stops to generate text. For example, \"User:\" can be used to stop generation when it's the user's turn to speak. Also known as Reverse prompts or Stop keywords. Add Antiprompt as Prefix : Whether to add the first one of Antiprompts automaticcaly to the next prompt as prefix after LLM stops without antiprompt (for example, after token is generated). For example, when antiprompts = { \"User:\" } and LLM stopped with EOS, \"User:\" will be automatically added to the next prompt. Antiprompt_prefix : Prefix for antiprompt when bAddAntipromptAsPrefix = true and LLM stopped without an antiprompt. For example, \"\\n\" can be used to add a line break before \"User:\" added to the next prompt. Antiprompt_suffix : Prefix for antiprompt when bAddAntipromptAsPrefix = true and LLM stopped without an antiprompt. For example, \" \" can be used to add a space after \"User:\" added to the next prompt. Input Prefix : Prefix immediately before the user's input. Input Suffix : Suffix immediately after user's input. This is useful for adding an \"Assistant:\" prompt after the user's input. It's added after the new-line character (\\n) that's automatically added to the end of the user's input. Context Setting N Ctx : Context length (number of tokens) that the model can use when generating text. Increasing this allows responses to longer inputs and longer conversations. N Keep : How many of the Initial Prompt tokens to keep when the total input/output to the model exceeds the context length. If this value is greater than ( N Ctx - 64), it is automatically set to ( N Ctx - 64). -1 = Keep all tokens of the Initial Prompt N Predict : Maximum number of tokens to generate for one input -1 = No limit -2 = Until context is filled Token Discard Rule For example, if N Ctx = 512 \u3001 N Keep = -1 \u3001 N Predict = -1 and Initial Prompt is 300 tokens, the conversation will take place within 512 - 300 = 212 tokens. Each input from the user and each output from the LLM will consume these 212 tokens, and when the remaining amount becomes less than 32 tokens, some of the past input and output will be discarded. The number of tokens to be discarded is set by the following parameter. If Discard Rule = Fixed Ratio : (Number of tokens used so far - N Keep ) * Ratio will be discarded. If Discard Rule = Fixed Length : The number of tokens specified by Length will be discarded. Sampling Setting Settings to fine-tune the diversity, creativity, and quality of the generated text according to your needs. See the tooltip of each item or the comments in LocalLLMStruct.h for details. Performance Setting Settings to improve performance and memory usage of LLM models. For details, see the tooltip of each item or the comments in LocalLLMStruct.h . Encode Setting Settings for conversions to handle special characters such as line feeds in the prompts you enter into LLM. For details, see the tooltip of each item or the comments in LocalLLMStruct.h . 3. Start Generation Create a process that allows you to input a string, called a prompt, to tell LLM to start generating text. Specifically, call the Start Generation function. The text generation itself is executed asynchronously. See \"4. Get results\" for how to get the results of generation. You can call the Start Generation function at any time, even during text generation, to enter a new prompt. 4. Get results Create the following event of Local LLM component to get the result. On Processing event provides intermidiate result while the model is still generating response. On Processed event provides final result after the model finishes the response. The following results are passed: Local LLM Component : The component from which this event triggered Input : The input string actually processed by the LLM, with Prefix and Suffix added to the user's input. Output : The output string from the LLM. Found Antiprompt : Antiprompt found at the end of the output string from the LLM, if any. Empty if not found. Exit Code (On Processed only): The reason why text generation stopped. Others To change setting, call Change Setting after changing values of Local LLM Setting . All the current settings and conversation history are discarded. The model is re-initialized with the new settings. Call Stop Generation to terminate the text generation. To the Hisotry variable, the actual inputs and outputs processed by the LLM are written out.","title":"Basic usage"},{"location":"how-to-use/#basic-usage","text":"See Plugins > LocalLLM > BP > BP_LocalLLM_Example for a sample implementation. You can test it in sample maps located at Plugins > LocalLLM > Map .","title":"Basic usage"},{"location":"how-to-use/#1-add-a-component","text":"Create an actor blueprint. Add Local LLM component.","title":"1. Add a component"},{"location":"how-to-use/#2-edit-settings","text":"Select the Local LLM component and edit the following settings at Local LLM > Local LLM Setting in the Details tab. Model Setting Path Type : The path type of the model. Whether absolute path or relative to the project directories. Model Path : Specify the path to the gguf model. Execution device : Whether to use CPU or GPU. Number Of Gpu Layers : If using a GPU, specify the number of layers in the model to be run on the GPU. Prompt Setting Initial Prompt : Describe the role of the LLM and basic instructions. Talk History : Describe the history of the conversation so far. This is used when you want to resume a previous conversation without using the state save/load described in the next page. Both of these are \"the first prompt entered into the LLM\" when the LLM is initialized. The difference is that the Initial Prompt is retained throughout the conversation (in the default setting), while the Talk History can be discarded if the conversation becomes too long. This is because there is a limit to the length of the string (context length) that can be entered into the LLM. How information is retained and discarded is set in the Context Setting described later. Example of Initial Prompt Role play. You are a talented engineer. You speak as an \"Assistant\". You give appropriate advice in response to the statement of \"User\". Your advice should be brief and to the point. An example of a conversation is shown below. (Example) User: I'm getting an error message and the compile fails. Assistant: Read the error message first. Most things can be solved with that. User: I have no idea what it's saying! Assistant: Then Google the error message. You may be able to find a solution in the forum. User: OK! Assistant: It is also important to read the manual, if there is one! Let's get started. Interaction Setting Set how to process prompts for chat-like experience with LLMs. Templates for common settings can be selected at Interaction Template . Interaction Template = Interaction of Two : Automatically configures the settings for the conversation between two people, the User and the Assistant. The names of the User and AI Assistant must be set, and these must match the conversation examples, if any, described in the Prompt Setting . Interaction Template = Instruction : Configures the settings for Instruction format. Used for Alpaca models, etc. Interaction Template = ChatML : Configures the settings for ChatML format. Used for models trained with ChatML syntax. Manual settings without templates If Interaction Template = None , set the following values manually. Antiprompts : When LLM outputs one of these phrases, it stops to generate text. For example, \"User:\" can be used to stop generation when it's the user's turn to speak. Also known as Reverse prompts or Stop keywords. Add Antiprompt as Prefix : Whether to add the first one of Antiprompts automaticcaly to the next prompt as prefix after LLM stops without antiprompt (for example, after token is generated). For example, when antiprompts = { \"User:\" } and LLM stopped with EOS, \"User:\" will be automatically added to the next prompt. Antiprompt_prefix : Prefix for antiprompt when bAddAntipromptAsPrefix = true and LLM stopped without an antiprompt. For example, \"\\n\" can be used to add a line break before \"User:\" added to the next prompt. Antiprompt_suffix : Prefix for antiprompt when bAddAntipromptAsPrefix = true and LLM stopped without an antiprompt. For example, \" \" can be used to add a space after \"User:\" added to the next prompt. Input Prefix : Prefix immediately before the user's input. Input Suffix : Suffix immediately after user's input. This is useful for adding an \"Assistant:\" prompt after the user's input. It's added after the new-line character (\\n) that's automatically added to the end of the user's input. Context Setting N Ctx : Context length (number of tokens) that the model can use when generating text. Increasing this allows responses to longer inputs and longer conversations. N Keep : How many of the Initial Prompt tokens to keep when the total input/output to the model exceeds the context length. If this value is greater than ( N Ctx - 64), it is automatically set to ( N Ctx - 64). -1 = Keep all tokens of the Initial Prompt N Predict : Maximum number of tokens to generate for one input -1 = No limit -2 = Until context is filled Token Discard Rule For example, if N Ctx = 512 \u3001 N Keep = -1 \u3001 N Predict = -1 and Initial Prompt is 300 tokens, the conversation will take place within 512 - 300 = 212 tokens. Each input from the user and each output from the LLM will consume these 212 tokens, and when the remaining amount becomes less than 32 tokens, some of the past input and output will be discarded. The number of tokens to be discarded is set by the following parameter. If Discard Rule = Fixed Ratio : (Number of tokens used so far - N Keep ) * Ratio will be discarded. If Discard Rule = Fixed Length : The number of tokens specified by Length will be discarded. Sampling Setting Settings to fine-tune the diversity, creativity, and quality of the generated text according to your needs. See the tooltip of each item or the comments in LocalLLMStruct.h for details. Performance Setting Settings to improve performance and memory usage of LLM models. For details, see the tooltip of each item or the comments in LocalLLMStruct.h . Encode Setting Settings for conversions to handle special characters such as line feeds in the prompts you enter into LLM. For details, see the tooltip of each item or the comments in LocalLLMStruct.h .","title":"2. Edit settings"},{"location":"how-to-use/#3-start-generation","text":"Create a process that allows you to input a string, called a prompt, to tell LLM to start generating text. Specifically, call the Start Generation function. The text generation itself is executed asynchronously. See \"4. Get results\" for how to get the results of generation. You can call the Start Generation function at any time, even during text generation, to enter a new prompt.","title":"3. Start Generation"},{"location":"how-to-use/#4-get-results","text":"Create the following event of Local LLM component to get the result. On Processing event provides intermidiate result while the model is still generating response. On Processed event provides final result after the model finishes the response. The following results are passed: Local LLM Component : The component from which this event triggered Input : The input string actually processed by the LLM, with Prefix and Suffix added to the user's input. Output : The output string from the LLM. Found Antiprompt : Antiprompt found at the end of the output string from the LLM, if any. Empty if not found. Exit Code (On Processed only): The reason why text generation stopped.","title":"4. Get results"},{"location":"how-to-use/#others","text":"To change setting, call Change Setting after changing values of Local LLM Setting . All the current settings and conversation history are discarded. The model is re-initialized with the new settings. Call Stop Generation to terminate the text generation. To the Hisotry variable, the actual inputs and outputs processed by the LLM are written out.","title":"Others"},{"location":"install/","text":"Installation Purchase at UE Marketplace and install it. Create an Unreal Engine project. Open the project, open Edit > Plugins on the editor menu, enable Local LLM , and restart the project.","title":"Installation"},{"location":"install/#installation","text":"Purchase at UE Marketplace and install it. Create an Unreal Engine project. Open the project, open Edit > Plugins on the editor menu, enable Local LLM , and restart the project.","title":"Installation"},{"location":"system-requirement/","text":"System Requirements Supported Unreal Engine version: 5.4 Supported Platforms Windows 64bit Hardware Requirements CPU Requires a CPU that supports AVX, AVX2 and FMA. The following CPUs should work, but please run demo exe to check if your CPU is supported. Intel: 4th Generation (Haswell) and above AMD: All Ryzen series (Optional) GPU NVIDIA GPU that support CUDA 12.2 Works with CPU only GPU is optional and works fine with CPU only. Installing CUDA To use with GPU, the following version of CUDA is required to be installed. CUDA 12.2.0","title":"System Requirements"},{"location":"system-requirement/#system-requirements","text":"","title":"System Requirements"},{"location":"system-requirement/#supported-unreal-engine-version","text":"5.4","title":"Supported Unreal Engine version:"},{"location":"system-requirement/#supported-platforms","text":"Windows 64bit","title":"Supported Platforms"},{"location":"system-requirement/#hardware-requirements","text":"","title":"Hardware Requirements"},{"location":"system-requirement/#cpu","text":"Requires a CPU that supports AVX, AVX2 and FMA. The following CPUs should work, but please run demo exe to check if your CPU is supported. Intel: 4th Generation (Haswell) and above AMD: All Ryzen series","title":"CPU"},{"location":"system-requirement/#optional-gpu","text":"NVIDIA GPU that support CUDA 12.2 Works with CPU only GPU is optional and works fine with CPU only. Installing CUDA To use with GPU, the following version of CUDA is required to be installed. CUDA 12.2.0","title":"(Optional) GPU"}]}