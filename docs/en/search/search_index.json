{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Local LLM Plugin? Local LLM Plugin allows to load a large language model (LLM) of GGUF format and run it on the Unreal Engine. It works offline on a local PC. Just add one component to your BP and you are ready to use it. Multibyte characters are supported.","title":"What is Local LLM Plugin?"},{"location":"#what-is-local-llm-plugin","text":"Local LLM Plugin allows to load a large language model (LLM) of GGUF format and run it on the Unreal Engine. It works offline on a local PC. Just add one component to your BP and you are ready to use it. Multibyte characters are supported.","title":"What is Local LLM Plugin?"},{"location":"changelog/","text":"Changelog v1.0 (Jun. xx, 2024) First release.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v10-jun-xx-2024","text":"First release.","title":"v1.0 (Jun. xx, 2024)"},{"location":"demo/","text":"","title":"Demo"},{"location":"faq/","text":"FAQ What is the GGUF format? It is a model format used in a machine learning framework called GGML published by Georgi Gerganov. One of its features is that information required for language models, such as vocabulary definitions, is included in a single file, making it highly portable. For details, please see the official page .","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#what-is-the-gguf-format","text":"It is a model format used in a machine learning framework called GGML published by Georgi Gerganov. One of its features is that information required for language models, such as vocabulary definitions, is included in a single file, making it highly portable. For details, please see the official page .","title":"What is the GGUF format?"},{"location":"how-to-get-model/","text":"Get models This plugin uses GGUF format models. The following models have been tested to work. llama-3 7B Phi-3-medium Gemma 7B Mistral 7B Japanese model ArrowPro 7B KUJIRA","title":"Get models"},{"location":"how-to-get-model/#get-models","text":"This plugin uses GGUF format models. The following models have been tested to work. llama-3 7B Phi-3-medium Gemma 7B Mistral 7B Japanese model ArrowPro 7B KUJIRA","title":"Get models"},{"location":"how-to-use/","text":"Basic usage See Plugins > LocalLLM > Sample > BP > BP_Example for a sample implementation. You can test it in a sample map located at Plugins > LocalLLM > Sample > Map > Test . Setup Create an actor blueprint. Add Local LLM component. Select Local LLM component and set Local LLM > Prompt in Details tab. Select Local LLM component and set Local LLM > Local LLM Setting > Model Setting in Details tab. Model Path : Specify the path to the gguf model. Execution device : Whether to use CPU or GPU. Number Of Gpu Layers : If using a GPU, specify the number of layers in the model to be run on the GPU. Main GPU : If using a GPU and the PC is equipped with multiple GPUs, specify the index of the GPU to be used. Create On Generating event and On Generated event of Local LLM component to get the result. On Generating event provides intermidiate result while the model is still generating response. On Generated event provides final result after the model finishes the response. Change setting To change setting, call Change Setting after changing values of Local LLM Setting . Start and stop Call Start Generation to generate response to Prompt . Call Stop Generation to terminate the response to the Prompt .","title":"Basic usage"},{"location":"how-to-use/#basic-usage","text":"See Plugins > LocalLLM > Sample > BP > BP_Example for a sample implementation. You can test it in a sample map located at Plugins > LocalLLM > Sample > Map > Test .","title":"Basic usage"},{"location":"how-to-use/#setup","text":"Create an actor blueprint. Add Local LLM component. Select Local LLM component and set Local LLM > Prompt in Details tab. Select Local LLM component and set Local LLM > Local LLM Setting > Model Setting in Details tab. Model Path : Specify the path to the gguf model. Execution device : Whether to use CPU or GPU. Number Of Gpu Layers : If using a GPU, specify the number of layers in the model to be run on the GPU. Main GPU : If using a GPU and the PC is equipped with multiple GPUs, specify the index of the GPU to be used. Create On Generating event and On Generated event of Local LLM component to get the result. On Generating event provides intermidiate result while the model is still generating response. On Generated event provides final result after the model finishes the response.","title":"Setup"},{"location":"how-to-use/#change-setting","text":"To change setting, call Change Setting after changing values of Local LLM Setting .","title":"Change setting"},{"location":"how-to-use/#start-and-stop","text":"Call Start Generation to generate response to Prompt . Call Stop Generation to terminate the response to the Prompt .","title":"Start and stop"},{"location":"install/","text":"Installation Purchase at UE Marketplace and install it. Create an Unreal Engine project. Open the project, open Edit > Plugins on the editor menu, enable Local LLM , and restart the project.","title":"Installation"},{"location":"install/#installation","text":"Purchase at UE Marketplace and install it. Create an Unreal Engine project. Open the project, open Edit > Plugins on the editor menu, enable Local LLM , and restart the project.","title":"Installation"},{"location":"system-requirement/","text":"System Requirements Supported Unreal Engine version: 5.4 Supported Platforms Windows 64bit Supported GPU NVIDIA GPUs that support CUDA 12.2","title":"System Requirements"},{"location":"system-requirement/#system-requirements","text":"","title":"System Requirements"},{"location":"system-requirement/#supported-unreal-engine-version","text":"5.4","title":"Supported Unreal Engine version:"},{"location":"system-requirement/#supported-platforms","text":"Windows 64bit","title":"Supported Platforms"},{"location":"system-requirement/#supported-gpu","text":"NVIDIA GPUs that support CUDA 12.2","title":"Supported GPU"}]}